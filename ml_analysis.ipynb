{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2c7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, LinearSVC, DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"team_comp_and_analysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afceb088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.csv(\"*.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null_rows_count = df.filter(\n",
    "#     F.expr(\" OR \".join([f\"{col} IS NULL\" for col in df.columns]))\n",
    "# ).count()\n",
    "\n",
    "# players_per_team_round = (\n",
    "#     df.groupBy(\"matchid\", \"roundnumber\", \"team\")\n",
    "#       .agg(F.count(\"*\").alias(\"player_count\"))\n",
    "# )\n",
    "\n",
    "# matchids_over_5 = (\n",
    "#     players_per_team_round.filter(F.col(\"player_count\") > 5)\n",
    "#                           .select(\"matchid\")\n",
    "#                           .distinct()\n",
    "# )\n",
    "\n",
    "# df_filtered = df.join(matchids_over_5, on=\"matchid\", how=\"left_anti\")\n",
    "\n",
    "# first_round_counts = (\n",
    "#     df_filtered.filter(F.col(\"roundnumber\") == 1)\n",
    "#                .groupBy(\"matchid\", \"team\")\n",
    "#                .agg(F.count(\"*\").alias(\"player_count\"))\n",
    "# )\n",
    "\n",
    "# match_teams = df_filtered.select(\"matchid\", \"team\").distinct()\n",
    "\n",
    "# missing_in_first_round = (\n",
    "#     match_teams.join(first_round_counts, on=[\"matchid\", \"team\"], how=\"left_anti\")\n",
    "#                 .select(\"matchid\").distinct()\n",
    "# )\n",
    "\n",
    "# df_clear = df_filtered.join(missing_in_first_round, on=\"matchid\", how=\"left_anti\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4666fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_selected = df_clear.select('gamemode', 'mapname', 'operator', 'skillrank', 'haswon', 'matchid', 'roundnumber', 'role', 'clearancelevel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = spark.read.parquet(\"df_selected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(model_name, train_data, test_data):\n",
    "    label_column = \"haswon\"\n",
    "    indexers = [StringIndexer(inputCol=\"gamemode\", outputCol=\"gamemode_idx\", handleInvalid=\"keep\"),\n",
    "                StringIndexer(inputCol=\"mapname\", outputCol=\"mapname_idx\", handleInvalid=\"keep\"),\n",
    "                StringIndexer(inputCol=\"operator\", outputCol=\"operator_idx\", handleInvalid=\"keep\"),\n",
    "                StringIndexer(inputCol=\"role\", outputCol=\"role_idx\", handleInvalid=\"keep\"),\n",
    "                StringIndexer(inputCol=\"skillrank\", outputCol=\"skillrank_idx\", handleInvalid=\"keep\"),]\n",
    "    encoders = [OneHotEncoder(inputCol=\"gamemode_idx\", outputCol=\"gamemode_vec\"),\n",
    "                OneHotEncoder(inputCol=\"mapname_idx\", outputCol=\"mapname_vec\"),\n",
    "                OneHotEncoder(inputCol=\"operator_idx\", outputCol=\"operator_vec\"),\n",
    "                OneHotEncoder(inputCol=\"role_idx\", outputCol=\"role_vec\"),\n",
    "                OneHotEncoder(inputCol=\"skillrank_idx\", outputCol=\"skillrank_vec\"),]\n",
    "    feature_cols = [\"gamemode_vec\", \"mapname_vec\", \"operator_vec\", \"role_vec\", \"skillrank_vec\", \"clearancelevel\"]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    if model_name == \"RandomForest\":\n",
    "        classifier = RandomForestClassifier(labelCol=label_column, featuresCol=\"features\")\n",
    "    elif model_name == \"LogisticRegression\":\n",
    "        classifier = LogisticRegression(labelCol=label_column, featuresCol=\"features\")\n",
    "    elif model_name == \"LinearSVC\":\n",
    "        classifier = LinearSVC(labelCol=label_column, featuresCol=\"features\")\n",
    "    elif model_name == \"DecisionTree\":\n",
    "        classifier = DecisionTreeClassifier(labelCol=label_column, featuresCol=\"features\")\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler, classifier])\n",
    "    model = pipeline.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=label_column)\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    return predictions, auc\n",
    "    \n",
    "def metrics(predictions):\n",
    "    label_column = \"haswon\"\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=label_column , predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    conf_matrix = predictions.groupBy(label_column, \"prediction\").count()\n",
    "\n",
    "    conf_matrix = conf_matrix.toPandas()\n",
    "    conf_matrix_pivot = conf_matrix.pivot(index=label_column, columns='prediction', values='count')\n",
    "\n",
    "    tp = conf_matrix_pivot.loc[1, 1]\n",
    "    fp = conf_matrix_pivot.loc[0, 1]\n",
    "    tn = conf_matrix_pivot.loc[0, 0]\n",
    "    fn = conf_matrix_pivot.loc[1, 0]\n",
    "\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    fpr = fp / (fp + tn)\n",
    "    tnr = tn / (tn + fp)\n",
    "\n",
    "    print(\"Recall (Sensitivity): {:.4f}\".format(recall))\n",
    "    print(\"Positive Predictive Value (Precision): {:.4f}\".format(precision))\n",
    "    print(\"False Positive Rate (1 - Specificity): {:.4f}\".format(fpr))\n",
    "    print(\"True Negative Rate (Specificity): {:.4f}\".format(tnr))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix_pivot, annot=True, fmt='d', cmap='Blues',cbar= False)\n",
    "    plt.title(\"Confusion Matrix Heatmap\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6472e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_selected.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b4674",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc = analysis(\"RandomForest\", train, test)\n",
    "metrics(predictions)\n",
    "predictions = predictions.withColumn(\"prob_pos\", vector_to_array(\"probability\")[1])\n",
    "\n",
    "pdf = predictions.select(\"haswon\", \"prob_pos\").toPandas()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(pdf['haswon'], pdf['prob_pos'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc = analysis(\"LogisticRegression\", train, test)\n",
    "metrics(predictions)\n",
    "predictions = predictions.withColumn(\"prob_pos\", vector_to_array(\"probability\")[1])\n",
    "\n",
    "pdf = predictions.select(\"haswon\", \"prob_pos\").toPandas()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(pdf['haswon'], pdf['prob_pos'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7eb193",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc = analysis(\"LinearSVC\", train, test)\n",
    "metrics(predictions)\n",
    "predictions = predictions.withColumn(\"prob_pos\", vector_to_array(\"probability\")[1])\n",
    "\n",
    "pdf = predictions.select(\"haswon\", \"prob_pos\").toPandas()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(pdf['haswon'], pdf['prob_pos'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3100618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, auc = analysis(\"DecisionTree\", train, test)\n",
    "metrics(predictions)\n",
    "predictions = predictions.withColumn(\"prob_pos\", vector_to_array(\"probability\")[1])\n",
    "\n",
    "pdf = predictions.select(\"haswon\", \"prob_pos\").toPandas()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(pdf['haswon'], pdf['prob_pos'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0,1.0])\n",
    "plt.ylim([0.0,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
